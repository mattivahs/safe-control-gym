'''Dynamics learning with PID control class for Crazyflies.

Represent Dynamics as neural net and learn online from data. Data is simply generated by existing PID controller.
'''

import math
import os

import numpy as np
import pybullet as p
from scipy.spatial.transform import Rotation
import torch

from safe_control_gym.controllers.pid.pid import PID
from safe_control_gym.envs.benchmark_env import Environment, Task
from safe_control_gym.controllers.pid_rl.pidrl_utils import *

device = 'cuda:0' if torch.cuda.is_available() else 'cpu'

class PIDRL(PID):
    '''PID RL class'''

    def __init__(self,
                 env_func=None,
                 n_episodes=10,
                 n_steps=300,
                 **kwargs
                 ):
        '''Common control classes __init__ method.'''

        super().__init__(env_func, **kwargs)

        self.dynModel = Drone2DModel().to(device)
        self.buffer = ReplayBuffer(100000)
        self.dataset = RLDataset(self.buffer)
        self.trainer = Trainer(self.dynModel, self.dataset)

        self.n_episodes = n_episodes
        self.n_steps = n_steps
        self.loss_list = []

    def save(self,
             path
             ):
        '''Saves model params.'''
        self.dynModel.save(path)

    def load(self,
             path
             ):
        '''Restores model given checkpoint path.'''
        self.dynModel.load(path)

    def train_callback(self, ep, loss):
        self.loss_list.append(loss)
        print("Epoch: {}, Loss: {}".format(ep, loss))
    def learn(self,
              env=None,
              **kwargs
              ):
        '''Performs learning (pre-training, training, fine-tuning, etc).'''
        train_losses = []
        for k in range(self.n_episodes):
            obs, info = env.reset()
            ep_ret = 0.
            if k > 0 and False:
                # train
                loss = self.trainer.train()
                train_losses.append(loss)
                print(loss)
            for _ in range(self.n_steps):
                act = torch.tensor(self.select_action(obs), device=device, dtype=torch.float32)

                # state s
                s = obs2state(obs)

                # modified for Safe RL, added cost
                obs, reward, terminated, info = env.step(act.cpu())

                # state s prime
                sp = obs2state(obs)
                self.buffer.append((s.cpu().numpy(), act.cpu(), sp.cpu().numpy()))

                ep_ret += reward
                if terminated:
                    observation, info = env.reset()
        loss = self.trainer.train(n_epochs=200, batch_size=16, lr=1e-3, cb_fun=self.train_callback, weight_decay=0.00001)
        print(loss)