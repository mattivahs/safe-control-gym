'''Dynamics learning with PID control class for Crazyflies.

Represent Dynamics as neural net and learn online from data. Data is simply generated by existing PID controller.
'''

import time

from safe_control_gym.controllers.base_controller import BaseController
from safe_control_gym.envs.benchmark_env import Environment, Task
from safe_control_gym.controllers.neuralmpc.neural_mpc_utils import *

device = 'cpu'#'cuda:0' if torch.cuda.is_available() else 'cpu'
import casadi as cs
import l4casadi as l4c
from copy import deepcopy

class NeuralMPC(BaseController):
    '''PID RL class'''

    def __init__(self,
                 env_func=None,
                 n_episodes=3,
                 n_steps=300,
                 **config
                 ):
        '''Common control classes __init__ method.'''

        super().__init__(env_func, **config)

        self.buffer = ReplayBuffer(100000)
        self.dataset = RLDataset(self.buffer)
        self.env = env_func
        self.dt = 1 / self.env.CTRL_FREQ


        self.prior_model = self.get_prior(self.env)
        self.model = LearnedDynamics(env_func, dt=self.dt)
        self.model_cs = l4c.realtime.RealTimeL4CasADi(self.model.dyn, device=device, approximation_order=2)
        # self.model_cs = l4c.L4CasADi(self.model.dyn, device=device, model_expects_batch_dim=True)
        self.horizon = config['horizon']
        self.warmstart = config['warmstart']

        self.task = self.env.TASK

        self.reference = np.empty(0)

        self.Q = np.diag(np.ones(self.model.dyn.nx))
        self.R = np.diag(np.ones(self.model.dyn.nu))

        self.x_prev = None
        self.u_prev = None
        self.prev_action = None

        # Setup optimizer
        self.setup_optimizer()
    def save(self,
             path
             ):
        '''Saves model params.'''
        self.model.dyn.save(path)

    def load(self,
             path
             ):
        '''Restores model given checkpoint path.'''
        self.model.dyn.load(path)

    def close(self):
        '''Cleans up resources.'''
        self.env.close()

    def reset(self):
        '''Prepares for training or evaluation.'''
        # Setup reference input.
        if self.env.TASK == Task.STABILIZATION:
            self.mode = 'stabilization'
            self.x_goal = self.env.X_GOAL
        elif self.env.TASK == Task.TRAJ_TRACKING:
            self.mode = 'tracking'
            self.traj = self.env.X_GOAL.T
            # Step along the reference.
            self.traj_step = 0
        self.setup_results_dict()

    def setup_results_dict(self):
        '''Setup the results dictionary to store run information.'''
        self.results_dict = {'obs': [],
                             'reward': [],
                             'done': [],
                             'info': [],
                             'action': [],
                             'horizon_inputs': [],
                             'horizon_states': [],
                             'goal_states': [],
                             'frames': [],
                             'state_mse': [],
                             'common_cost': [],
                             'state': [],
                             'state_error': [],
                             't_wall': []
                             }

    def train_callback(self, ep, loss):
        self.loss_list.append(loss)
        print("Epoch: {}, Loss: {}".format(ep, loss))

    def learn(self,
              env=None,
              safety_filter=None,
              **kwargs
              ):
        '''Performs learning (pre-training, training, fine-tuning, etc).'''
        train_losses = []
        for k in range(self.n_episodes):
            obs, info = env.reset(seed=46)
            ep_ret = 0.
            if k > 0:
                # train
                loss = self.trainer.train(n_epochs=200, batch_size=16, lr=1e-3, cb_fun=self.train_callback,
                                  weight_decay=0.00001)
                train_losses.append(loss)
                print(loss)
            for _ in range(self.n_steps):
                act = torch.tensor(self.select_action(obs, info), device=device, dtype=torch.float32)

                # state s
                s = self.dyn.obs2state(obs)

                if safety_filter is not None:
                    act, _ = safety_filter.certify_action(obs, act.cpu().numpy(), info)
                    act = act.astype(np.float32)
                else:
                    act = act.cpu().numpy()
                # modified for Safe RL, added cost
                obs, reward, terminated, info = env.step(act)

                # state s prime
                sp = self.dyn.obs2state(obs)
                self.buffer.append((s.cpu().numpy(), act, sp.cpu().numpy()))

                ep_ret += reward
                if terminated:
                    break
        loss = self.trainer.train(n_epochs=200, batch_size=16, lr=1e-3, cb_fun=self.train_callback,
                                  weight_decay=0.00001)
        print(loss)

    def setup_optimizer(self):
        '''Sets up nonlinear optimization problem.'''
        nx, nu = self.model.dyn.nx, self.model.dyn.nu
        T = self.horizon

        # Define optimizer and variables.
        opti = cs.Opti()
        # States.
        x_var = opti.variable(nx, T + 1)
        # Inputs.
        u_var = opti.variable(nu, T)
        # Initial state.
        x_init = opti.parameter(nx, 1)
        # Reference (equilibrium point or trajectory, last step for terminal cost).
        x_ref = opti.parameter(nx, T + 1)
        # Add slack variables
        # state_slack = opti.variable(len(self.state_constraints_sym))
        # input_slack = opti.variable(len(self.input_constraints_sym))

        # cost (cumulative)
        cost = 0
        cost_func = self.prior_model.loss
        for i in range(T):
            # Can ignore the first state cost since fist x_var == x_init.
            cost += CostMPC(x=x_var[:, i],
                            x_ref=x_ref[:, i],
                            u=u_var[:, i],
                            u_ref=np.zeros((nu, 1)),
                            Q=self.Q,
                            R=self.R)
        # Terminal cost.
        cost += CostMPC(x=x_var[:, -1],
                        x_ref=x_ref[:, -1],
                        u=u_var[:, -1],
                        u_ref=np.zeros((nu, 1)),
                        Q=self.Q,
                        R=self.R)

        # Constraints
        for i in range(self.horizon):
            # Dynamics constraints.
            next_state = self.model_cs(cs.vertcat(x_var[:, i], u_var[:, i]))
            opti.subject_to(x_var[:, i + 1] == next_state)

        # initial condition constraints
        opti.subject_to(x_var[:, 0] == x_init)

        opti.minimize(cost)
        # Create solver (IPOPT solver in this version)
        # opts = {'ipopt.print_level': 0, 'ipopt.sb': 'yes', 'print_time': 0}
        opts = {'expand': True}
        opti.solver('ipopt', opts)
        self.opti_dict = {
            'opti': opti,
            'x_var': x_var,
            'u_var': u_var,
            'x_init': x_init,
            'x_ref': x_ref,
            'cost': cost
        }

    def construct_problem(self):
        nx, nu = self.model.dyn.nx, self.model.dyn.nu
        N = self.horizon

        # states
        x = cs.MX.sym("x", N, nx)
        u = cs.MX.sym("u", N - 1, nu)
        params = cs.MX.sym("params", nx * (N + 1) + 29 * (N-1), 1)
        params1 = params[:nx * (N + 1)].reshape((N + 1, nx))
        x_init = params1[0, :]
        x_ref = params1[1:, :]
        taylor_params = params[(2 * (N + 1)):, :].reshape((N-1, 29))

        # Define NLP
        f = 0.
        g = []
        lbg = []
        ubg = []
        Q = self.Q
        R = self.R
        for k in range(N):
            # cost function
            if k < N - 2:
                f += u[k, :] @ R @ u[k, :].T + (x[k, :] - x_ref[k, :]) @ Q @ (x[k, :] - x_ref[k, :]).T

            # state constraints
            g = cs.horzcat(g, x[k, :])
            lbg = cs.horzcat(lbg, cs.DM([-2., 0., -1., -1., -100., -100., -100.]).T)
            ubg = cs.horzcat(ubg, cs.DM([2., 2., 1., 1., 100., 100., 100.]).T)

            # dynamics constraints
            if k+1 < N:
                # next_state = self.model(cs.vertcat(x[k, :].T, u[k, :]))
                g = cs.horzcat(g, x[k+1, :] - self.realtime_model(cs.vertcat(x[k, :].T, u[k, :]), taylor_params[k, :]).T)
                lbg = cs.horzcat(lbg, cs.DM([0., 0., 0., 0., 0., 0., 0.]).T)
                ubg = cs.horzcat(ubg, cs.DM([0., 0., 0., 0., 0., 0., 0.]).T)

            # initial state cosntraint
            if k == 0:
                g = cs.horzcat(g, x[k, :] - x_init)
                lbg = cs.horzcat(lbg, cs.DM([0., 0., 0., 0., 0., 0., 0.]).T)
                ubg = cs.horzcat(ubg, cs.DM([0., 0., 0., 0., 0., 0., 0.]).T)

            # input constraints
            if k < N - 1:
                # control limits
                g = cs.horzcat(g, u[k, :])
                lbg = cs.horzcat(lbg, cs.DM(-2))
                ubg = cs.horzcat(ubg, cs.DM(2))


        # Generate solver
        x_nlp = cs.vertcat(cs.reshape(x, self.N * 2, 1), u)
        # params_nlp = cs.reshape(params, (self.N + 1) * 2, 1)
        # taylor_p_nlp = cs.reshape(taylor_params, 29 * (self.N - 1), 1)
        p_nlp = params# cs.vertcat(params_nlp, taylor_p_nlp)
        nlp_dict = {
            "x": x_nlp,
            "f": f,
            "g": g,
            "p": p_nlp,
        }
        nlp_opts = {
            "ipopt.linear_solver": "mumps",
            "ipopt.sb": "yes",
            "ipopt.max_iter": 100,
            "ipopt.tol": 1e-4,
            "ipopt.print_level": 1,
            "print_time": False,
        }
        nlp_solver = cs.nlpsol("trajectory_generator", "ipopt", nlp_dict, nlp_opts)

        solver = {"solver": nlp_solver, "lbg": lbg, "ubg": ubg}

        return solver

    def get_references(self):
        '''Constructs reference states along mpc horizon.(nx, T+1).'''
        if self.env.TASK == Task.STABILIZATION:
            # Repeat goal state for horizon steps.
            goal_states = np.tile(self.model.dyn.obs2state(self.env.X_GOAL).cpu().numpy().reshape(-1, 1), (1, self.horizon + 1))
        elif self.env.TASK == Task.TRAJ_TRACKING:
            # Slice trajectory for horizon steps, if not long enough, repeat last state.
            start = min(self.traj_step, self.traj.shape[-1])
            end = min(self.traj_step + self.horizon + 1, self.traj.shape[-1])
            remain = max(0, self.horizon + 1 - (end - start))
            goal_states = np.concatenate([
                self.traj[:, start:end],
                np.tile(self.traj[:, -1:], (1, remain))
            ], -1)
        else:
            raise Exception('Reference for this mode is not implemented.')
        return goal_states  # (nx, T+1).

    def select_action(self,
                      obs,
                      info=None
                      ):
        '''Solves nonlinear mpc problem to get next action.

        Args:
            obs (ndarray): Current state/observation.
            info (dict): Current info

        Returns:
            action (ndarray): Input/action to the task/env.
        '''
        opti_dict = self.opti_dict
        opti = opti_dict['opti']
        x_var = opti_dict['x_var']
        u_var = opti_dict['u_var']
        x_init = opti_dict['x_init']
        x_ref = opti_dict['x_ref']

        # Assign the initial state.
        s = self.model.dyn.obs2state(obs).cpu().numpy().reshape(-1, 1)
        opti.set_value(x_init, s)
        # Assign reference trajectory within horizon.
        goal_states = self.get_references()
        opti.set_value(x_ref, goal_states)
        if self.mode == 'tracking':
            self.traj_step += 1

        if self.warmstart and self.x_prev is not None and self.u_prev is not None:
            # shift previous solutions by 1 step
            x_guess = deepcopy(self.x_prev)
            u_guess = deepcopy(self.u_prev)
            x_guess[:, :-1] = x_guess[:, 1:]
            u_guess[:-1] = u_guess[1:]
            opti.set_initial(x_var, x_guess)
            opti.set_initial(u_var, u_guess)
        # Solve the optimization problem.
        sol = opti.solve()
        x_val, u_val = sol.value(x_var), sol.value(u_var)
        self.x_prev = x_val
        self.u_prev = u_val
        self.results_dict['horizon_states'].append(deepcopy(self.x_prev))
        self.results_dict['horizon_inputs'].append(deepcopy(self.u_prev))
        self.results_dict['goal_states'].append(deepcopy(goal_states))
        self.results_dict['t_wall'].append(opti.stats()['t_wall_total'])
        # Take the first action from the solved action sequence.
        if u_val.ndim > 1:
            action = u_val[:, 0]
        else:
            action = np.array([u_val[0]])
        self.prev_action = action
        return action

